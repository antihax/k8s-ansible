## cloud hosts are configured with cloud-init via hetzner to use dhcp.
## we must remove the automatic configuration to add our router.
- hosts: cloud
  tasks:
  - name: configure static networking and gateway
    template:
      src: cloud/etc/netplan/50-cloud-init.yaml.j2
      dest: /etc/netplan/50-cloud-init.yaml
      owner: root
      group: root
      mode: 0644
    notify:
      - netplan apply
  - name: disable cloud-init network
    template:
      src: cloud/etc/cloud/cloud.cfg.d/99-disable-network-config.cfg.j2
      dest: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg
      owner: root
      group: root
      mode: 0644      
    notify:
      - netplan apply
  handlers:
  - name: netplan apply
    command: netplan apply

### setup the physicalworkers servers
- hosts: physicalworkers
  become: true
  tasks:

  - lineinfile:
      path: /etc/apt/apt.conf.d/99force-ipv6
      line: 'Acquire::ForceIPv6 "true";'
      create: yes
      mode: 0644

  - name: install baseline packages
    apt:
      name: "{{ packages }}"
      state: present
      update_cache: yes
    vars:
      packages:
      - linux-generic-hwe-{{ ansible_distribution_version }}
      - linux-generic
    notify: restart server

  handlers:
    - name: restart server
      ansible.builtin.reboot:
        reboot_timeout: 120

- hosts: kubernetes
  become: true
  roles:
    -  { role: ubuntu }
    -  { role: k8s }

### setup the master servers
- hosts: masters
  become: true

  tasks:
  - name: apt signing key for helm 
    apt_key:
      url: https://baltocdn.com/helm/signing.asc
      state: present

  - name: apt repository for helm
    apt_repository:
      repo: deb [arch=amd64] https://baltocdn.com/helm/stable/debian/ all main
      state: present

  - name: install helm
    apt:
      name: helm
      state: latest
      update_cache: yes   

  - name: install kubectl
    apt:
      name: kubectl
      state: latest

  - name: create kubeadm config
    template:
      src: kubeadm-init.yaml.j2
      dest: ~/kubeadm-init.yaml
      owner: root
      group: root
      mode: 0600

  - name: check if already initilized
    file:
      path: /etc/kubernetes/admin.conf
      state: file
    register: kubeadm_init
    failed_when: false

  - name: initialize the cluster
    shell: kubeadm init --config ~/kubeadm-init.yaml
    when: kubeadm_init.uid is undefined

  - name: check if /etc/kubernetes/admin.conf exists
    stat: path=/etc/kubernetes/admin.conf
    register: k8s_admin_conf
    failed_when: not k8s_admin_conf.stat.exists

  - name: create .kube dir
    file:
      path: ~/.kube/
      state: directory
    when: k8s_admin_conf.stat.exists

  - name: copy admin.conf to root kube config
    copy:
      src: /etc/kubernetes/admin.conf
      dest: ~/.kube/config
      mode: '0600'
      remote_src: yes
    when: k8s_admin_conf.stat.exists

  - name: copy admin.conf to user's kube config
    fetch:
      src: /etc/kubernetes/admin.conf
      dest: ~/.kube/config
      remote_src: yes
      flat: yes
    when: k8s_admin_conf.stat.exists

  - name: create calico-resources.yaml config
    template:
      src: calico-resources.yaml.j2
      dest: ~/calico-resources.yaml
      owner: root
      group: root
      mode: 0600

  - name: download calico config
    get_url:
      url: https://projectcalico.docs.tigera.io/manifests/tigera-operator.yaml
      dest: ~/tigera-operator.yaml
      mode: '0600'

  - name: deploy calico tigera
    kubernetes.core.k8s:
      state: present
      src: ~/tigera-operator.yaml

  - name: deploy calico resources
    kubernetes.core.k8s:
      state: present
      src: ~/calico-resources.yaml

  - name: add haproxytech helm repo
    kubernetes.core.helm_repository:
      name: haproxytech
      repo_url: https://haproxytech.github.io/helm-charts

  - name: install helm-diff plugin
    kubernetes.core.helm_plugin:
      plugin_path: https://github.com/databus23/helm-diff
      state: present

  - name: create haproxy namespace
    kubernetes.core.k8s:
      name: haproxy
      kind: Namespace
      state: present

  - name: deploy haproxy ingress
    kubernetes.core.helm:
      name: haproxy
      chart_ref: haproxytech/kubernetes-ingress
      release_namespace: haproxy
      release_state: present
      values:
        controller:
          kind: DaemonSet
          daemonset:
            useHostPort: true
          nodeSelector:
            loadbalancer: "haproxy"

  - name: create longhorn namespace
    kubernetes.core.k8s:
      name: longhorn-system
      kind: Namespace
      state: present

  - name: add longhorn helm repo
    kubernetes.core.helm_repository:
      name: longhorn
      repo_url: https://charts.longhorn.io

  - name: deploy longhorn 
    kubernetes.core.helm:
      name: longhorn
      chart_ref: longhorn/longhorn
      release_namespace: longhorn-system

  - name: download cert-manager config
    get_url:
      url: https://github.com/jetstack/cert-manager/releases/latest/download/cert-manager.yaml
      dest: ~/cert-manager.yaml
      mode: '0600'

  - name: deploy confguration
    kubernetes.core.k8s:
      src: "{{ item }}"
      state: present
    loop:
      - "~/cert-manager.yaml"


### join workers
- hosts: workers
  become: true

  tasks:
  - name: check if already initilized
    file:
      path: /etc/kubernetes/kubelet.conf
      state: file
    register: kubeadm_join
    failed_when: false
    tags: ["debug"]

  - name: get join token
    command: "kubeadm token create --ttl 1m --print-join-command"
    register: join_command
    delegate_to: master01
    when: kubeadm_join.uid is undefined
    tags: ["debug"]
  - set_fact:
      join_token: "{{ join_command.stdout | regex_search(token_regex, '\\2') | first }}"
      ca_cert_hash:  "{{ join_command.stdout | regex_search(ca_cert_hash_regex, '\\2') | first }}"
    vars:
      token_regex: '([^\s]+\s){4}([^\s]+)'
      ca_cert_hash_regex: '([^\s]+\s){6}([^\s]+)'
    when: kubeadm_join.uid is undefined
    tags: ["debug"]

  - name: create kubeadm config
    template:
      src: kubeadm-join.yaml.j2
      dest: ~/kubeadm-join.yaml
      owner: root
      group: root
      mode: 0600
      force: yes
    when: kubeadm_join.uid is undefined
    tags: ["debug"]

  - name: initialize the cluster
    shell: kubeadm join --config ~/kubeadm-join.yaml
    when: kubeadm_join.uid is undefined
    tags: ["debug"]
    
  - name: label nodes
    delegate_to: master01
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: v1
        kind: Node
        metadata:
          name: "{{ inventory_hostname }}"
          labels: 
            worker: "worker"
    tags: ["debug"]

### handle storage which double as loadbalancers
- hosts: physicalworkers
  become: true
  tasks:
  - name: label nodes
    delegate_to: master01
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: v1
        kind: Node
        metadata:
          name: "{{ inventory_hostname }}"
          labels: 
            loadbalancer: "haproxy"
    tags: ["debug"]